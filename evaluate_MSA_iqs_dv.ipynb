{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate MSA finetuning on iqs_dv\n",
    "A notebook that evaluates a fine tuned MSA model on the EVICAN test sets. Metrics are saved to a .json file and Model predictions are saved as plots to .pngs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Prompting imports\n",
    "from prompter import *\n",
    "\n",
    "# Metric imports\n",
    "from torchmetrics import Accuracy, Precision, Recall, JaccardIndex, F1Score\n",
    "from metrics import MeanIoU, PanopticQuality\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "calc_iou_matrix = MeanIoU._calc_iou_matrix\n",
    "\n",
    "# Augmentation imports\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "# Model\n",
    "from models.sam import SamPredictor, sam_model_registry\n",
    "\n",
    "# Data\n",
    "from dataset import *\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the original SAM weights here. \n",
    "# They can be downloaded from: \n",
    "# https://github.com/facebookresearch/segment-anything#model-checkpoints\n",
    "SAM_CKPT = '/home/zozchaab/checkpoint/sam/sam_vit_b_01ec64.pth'\n",
    "\n",
    "# Put your trained SAM model here\n",
    "TUNED_MODEL_CKPT = '/home/zozchaab/Medical-SAM-Adapter/logs/random_sampling_per_component_2024_01_14_21_06_28/Model/checkpoint_best.pth'\n",
    "\n",
    "# Data directory needed, where EVICAN should be saved to /loaded from\n",
    "DATA_DIR = '/home/zozchaab/data'\n",
    "\n",
    "# Define your save directory here\n",
    "SAVE_ROOT = '/home/zozchaab/Medical_SAM_Adapter_Training/Evaluation/iqs_dv'\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='GeForce RTX 3090', major=8, minor=6, total_memory=24268MB, multi_processor_count=82)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure GPU is available\n",
    "device = 0\n",
    "torch.cuda.get_device_properties(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please choose the checkpoint to be used here !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to circumvent the argparser in the original repo\n",
    "class Args:\n",
    "    thd = False\n",
    "    image_size = 120\n",
    "    crop_size = 120\n",
    "    data_path = \"/home/zozchaab/data/deepvision/deepvision\"\n",
    "    b = 2\n",
    "    w=0\n",
    "args = Args()\n",
    "\n",
    "#tuned_ckpt = torch.load(TUNED_MODEL_CKPT)\n",
    "sam = sam_model_registry['vit_b'](args, checkpoint=SAM_CKPT)\n",
    "#sam.load_state_dict(tuned_ckpt['state_dict'])\n",
    "sam.eval()\n",
    "sam = sam.to(device)\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transform_2d = transforms.Compose([\n",
    "\n",
    "lambda x: x.expand(3, -1, -1),\n",
    "transforms.Lambda(lambda x: x / 65535.0)\n",
    "\n",
    "])\n",
    "\n",
    "test_dataset = iqs_dv(data_path=os.path.join(args.data_path,'iqs_dv_test'),crop_size=args.crop_size,transform_2D=transform_2d)\n",
    "\n",
    "nice_test_loader = DataLoader(\n",
    "test_dataset,\n",
    "batch_size=args.b,\n",
    "shuffle=False,\n",
    "num_workers=args.w,\n",
    "collate_fn=collate_fn\n",
    ") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_POINTS_MAX = 1 # Define the number of intial prompts generated using gaussian sampling\n",
    "N_MAX_ITER_PROMPTS = 9 # Define the number of iterative prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                           | 87/173 [06:26<06:21,  4.44s/img]\n"
     ]
    }
   ],
   "source": [
    "metrics_out = {}\n",
    "vis_path = \"home/zozchaab/vis\"\n",
    "# Iterate over the three dataset difficulties\n",
    "\n",
    "\n",
    "folder = os.path.join(SAVE_ROOT, f'{N_POINTS_MAX}_{N_MAX_ITER_PROMPTS}')\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Create metrics\n",
    "metrics = [\n",
    "    Accuracy(task='binary').to(device), \n",
    "    Precision(task='binary').to(device), \n",
    "    Recall(task='binary').to(device), \n",
    "    F1Score(task='binary').to(device), \n",
    "    JaccardIndex(task='binary').to(device),\n",
    "]\n",
    "miou = MeanIoU('optimal', False, False).to(device)\n",
    "pq = PanopticQuality().to(device)\n",
    "ind =0\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(test_dataset), unit='img') as pbar:\n",
    "        for pack in nice_test_loader:\n",
    "            preds = []\n",
    "            scores = []\n",
    "            prompts = []\n",
    "            original_preds = []\n",
    "            imgs = pack['image'].to(dtype = torch.float32, device = device)\n",
    "            targets = pack['label'].to(dtype = torch.float32, device = device)\n",
    "            names = pack['metadata']\n",
    "            \n",
    "            for img, mask in zip(imgs, targets):\n",
    "                img_emb = sam.image_encoder(img.unsqueeze(0))\n",
    "                # Create Prompts\n",
    "            \n",
    "                # Randomly sample number of prompts\n",
    "                #n_points = np.random.randint(1, N_POINTS_MAX)\n",
    "                #n_pos = np.random.randint(1, n_points) if n_points > 1 else 1\n",
    "                #n_neg = np.random.randint(0, n_points-n_pos) if (n_points - n_pos) > 0 else 0\n",
    "                n_neg = 0\n",
    "                n_pos = 5\n",
    "                pts, lbls = sample_from_mask(mask.squeeze(0), mode=\"random\", n_pos=n_pos,n_neg = n_neg) \n",
    "    \n",
    "\n",
    "                user_iter = 0 \n",
    "                # Randomly add pseudo user input \n",
    "                #user_iter = np.random.randint(N_MAX_ITER_PROMPTS)\n",
    "                for i in range(user_iter):\n",
    "                    # print(f'User interaction {i+1}/{user_iter}')\n",
    "                    \n",
    "                    # Set prompt\n",
    "                    prompt = (pts.unsqueeze(0).to(device), lbls.unsqueeze(0).to(device))\n",
    "                    se, de = sam.prompt_encoder(\n",
    "                        points=prompt,\n",
    "                        boxes=None,\n",
    "                        masks=None,\n",
    "                    ) # type: ignore\n",
    "                    \n",
    "                    # Predict Mask\n",
    "                    pred, _ = sam.mask_decoder(\n",
    "                        image_embeddings=img_emb,\n",
    "                        image_pe=sam.prompt_encoder.get_dense_pe(),  # type: ignore\n",
    "                        sparse_prompt_embeddings=se,\n",
    "                        dense_prompt_embeddings=de, \n",
    "                        multimask_output=False,\n",
    "                    ) # type: ignore\n",
    "                    # Compare Prediction to GT\n",
    "                    pred = F.interpolate(pred, mask.shape[-2:]) # This is a bit cumbersome, but the easiest fix for now\n",
    "                    pred = pred.squeeze() > 0 #check this!!!\n",
    "                    clusters = pred.cpu() != mask\n",
    "                    # Sample point from largest error cluster \n",
    "                    new_prompt = find_best_new_prompt(clusters)\n",
    "                    new_label = mask[new_prompt[0, 1], new_prompt[0, 0]].to(torch.int64)\n",
    "                    pts = torch.concatenate([pts, new_prompt])\n",
    "                    lbls = torch.concatenate([lbls, torch.tensor([new_label])])\n",
    "\n",
    "                # Final Mask inference\n",
    "                prompts.append([pts,lbls])\n",
    "                prompt = (pts.unsqueeze(0).to(device), lbls.unsqueeze(0).to(device))\n",
    "\n",
    "                # Set Prompt\n",
    "                \n",
    "                se, de = sam.prompt_encoder(\n",
    "                    points=prompt,\n",
    "                    boxes=None,\n",
    "                    masks=None,\n",
    "                ) # type: ignore\n",
    "\n",
    "                # Predict Mask\n",
    "                pred, score = sam.mask_decoder(\n",
    "                    image_embeddings=img_emb,\n",
    "                    image_pe=sam.prompt_encoder.get_dense_pe(),  # type: ignore\n",
    "                    sparse_prompt_embeddings=se,\n",
    "                    dense_prompt_embeddings=de, \n",
    "                    multimask_output=False,\n",
    "                ) # type: ignore\n",
    "                original_preds.append((pred.squeeze(0) > 0).float())\n",
    "                pred = F.interpolate(pred, mask.shape[-2:]).squeeze(0) # This is a bit cumbersome, but the easiest fix for now\n",
    "                preds.append((pred > 0).float())\n",
    "                scores.append(score)\n",
    "                \n",
    "                for m in metrics:\n",
    "                    m.update(pred, mask.to(torch.uint8))\n",
    "                pq.update(pred, mask)\n",
    "                miou.update(pred, mask)\n",
    "                \n",
    "            scores = torch.stack(scores)\n",
    "            \n",
    "           \n",
    "            vis = 0\n",
    "            if vis:\n",
    "                if ind % vis == 0:\n",
    "                    visualize_batch(imgs=imgs, masks=targets, pred_masks=preds, names=names, prompts=prompts,original_preds=original_preds,save_path=vis_path)\n",
    "            ind+=1\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "ms = {str(m): m.compute().item() for m in metrics}\n",
    "metrics_out = {\n",
    "    **ms,\n",
    "    'miou': {k:v.item() for k, v in miou.compute().items()},\n",
    "    'pq': {k:v.item() for k, v in pq.compute().items()},\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(os.path.join(SAVE_ROOT, f'MSA_{N_POINTS_MAX}_{N_MAX_ITER_PROMPTS}_metrics.json'), 'w') as f:\n",
    "    json.dump(metrics_out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BinaryAccuracy()': 0.8903571367263794,\n",
       " 'BinaryPrecision()': 0.03192711994051933,\n",
       " 'BinaryRecall()': 0.3515303134918213,\n",
       " 'BinaryF1Score()': 0.05853765457868576,\n",
       " 'BinaryJaccardIndex()': 0.03015132248401642,\n",
       " 'miou': {'mIoU_micro': 0.010290836072619538,\n",
       "  'mIoU_macro': 0.010290835984051228,\n",
       "  'n_instances': 20760,\n",
       "  'n_images': 20760},\n",
       " 'pq': {'panoptic_quality': 0.0001517037017038092,\n",
       "  'recognition_quality': 0.00028901733458042145,\n",
       "  'segmentation_quality': 0.0001517037017038092}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(image,crop_size,top_left_x,top_left_y):\n",
    "    # Crop the image\n",
    "    return image[top_left_y:top_left_y + crop_size, top_left_x:top_left_x + crop_size]\n",
    "\n",
    "def crop_image_and_mask(image, mask, crop_size):\n",
    "    # Pad or crop the image to the target size\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    if h < crop_size or w < crop_size:\n",
    "        # Calculate padding needed\n",
    "        pad_h = max(0, crop_size - h)\n",
    "        pad_w = max(0, crop_size - w)\n",
    "\n",
    "        # Calculate padding on each side\n",
    "        top_pad = pad_h // 2\n",
    "        bottom_pad = pad_h - top_pad\n",
    "        left_pad = pad_w // 2\n",
    "        right_pad = pad_w - left_pad\n",
    "\n",
    "        # Pad the image\n",
    "        image = F.pad(image, (left_pad, right_pad, top_pad, bottom_pad), mode='constant', value=0)\n",
    "        \n",
    "    elif h > crop_size or w > crop_size:\n",
    "        # Randomly choose the top-left corner of the crop\n",
    "        top_left_x = np.random.randint(0, w - crop_size + 1)\n",
    "        top_left_y = np.random.randint(0, h - crop_size + 1)\n",
    "  \n",
    "        cropped_image = crop(image, crop_size,top_left_x,top_left_y)\n",
    "        cropped_mask = crop(mask, crop_size, top_left_x, top_left_y)\n",
    "\n",
    "    return cropped_image.unsqueeze(0), cropped_mask.unsqueeze(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
