{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate MSA finetuning on iqs_dv\n",
    "A notebook that evaluates a fine tuned MSA model on the EVICAN test sets. Metrics are saved to a .json file and Model predictions are saved as plots to .pngs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-net NET] [-baseline BASELINE]\n",
      "                             [-seg_net SEG_NET] [-mod MOD]\n",
      "                             [-exp_name EXP_NAME] [-type TYPE] [-vis VIS]\n",
      "                             [-reverse REVERSE] [-pretrain PRETRAIN]\n",
      "                             [-val_freq VAL_FREQ] [-gpu GPU]\n",
      "                             [-gpu_device GPU_DEVICE] [-sim_gpu SIM_GPU]\n",
      "                             [-epoch_ini EPOCH_INI] [-image_size IMAGE_SIZE]\n",
      "                             [-out_size OUT_SIZE] [-patch_size PATCH_SIZE]\n",
      "                             [-dim DIM] [-depth DEPTH] [-heads HEADS]\n",
      "                             [-mlp_dim MLP_DIM] [-w W] [-b B] [-s S]\n",
      "                             [-warm WARM] [-lr LR] [-uinch UINCH]\n",
      "                             [-imp_lr IMP_LR] [-weights WEIGHTS]\n",
      "                             [-base_weights BASE_WEIGHTS]\n",
      "                             [-sim_weights SIM_WEIGHTS]\n",
      "                             [-distributed DISTRIBUTED] [-dataset DATASET]\n",
      "                             [-sam_ckpt SAM_CKPT] [-thd THD] [-chunk CHUNK]\n",
      "                             [-num_sample NUM_SAMPLE] [-roi_size ROI_SIZE]\n",
      "                             [-evl_chunk EVL_CHUNK] [-crop_size CROP_SIZE]\n",
      "                             [-data_path DATA_PATH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/zozchaab/.local/share/jupyter/runtime/kernel-008877c6-c98b-4373-9819-74a97ce7801b.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zozchaab/anaconda3/envs/msa_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Prompting imports\n",
    "import prompter\n",
    "\n",
    "# Metric imports\n",
    "from torchmetrics import Accuracy, Precision, Recall, JaccardIndex, F1Score\n",
    "from metrics import MeanIoU, PanopticQuality\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "calc_iou_matrix = MeanIoU._calc_iou_matrix\n",
    "\n",
    "# Augmentation imports\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "# Model\n",
    "from models.sam import SamPredictor, sam_model_registry\n",
    "\n",
    "# Data\n",
    "from dataset import *\n",
    "from torch.utils.data import DataLoader\n",
    "from prompter import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the original SAM weights here. \n",
    "# They can be downloaded from: \n",
    "# https://github.com/facebookresearch/segment-anything#model-checkpoints\n",
    "SAM_CKPT = '/home/zozchaab/checkpoint/sam/sam_vit_b_01ec64.pth'\n",
    "\n",
    "# Put your trained SAM model here\n",
    "TUNED_MODEL_CKPT = '/home/zozchaab/Medical-SAM-Adapter/logs/random_sampling_per_component_2024_01_14_21_06_28/Model/checkpoint_best.pth'\n",
    "\n",
    "# Data directory needed, where EVICAN should be saved to /loaded from\n",
    "DATA_DIR = '/home/zozchaab/data'\n",
    "\n",
    "# Define your save directory here\n",
    "SAVE_ROOT = '/home/zozchaab/Medical_SAM_Adapter_Training/Evaluation/iqs_dv'\n",
    "os.makedirs(SAVE_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure GPU is available\n",
    "device = 0\n",
    "torch.cuda.get_device_properties(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is needed to circumvent the argparser in the original repo\n",
    "class Args:\n",
    "    thd = False\n",
    "    image_size = 120\n",
    "args = Args()\n",
    "\n",
    "tuned_ckpt = torch.load(TUNED_MODEL_CKPT)\n",
    "sam = sam_model_registry['vit_b'](args, checkpoint=SAM_CKPT) #choose which checkpoint!!!\n",
    "sam.load_state_dict(tuned_ckpt['state_dict'])\n",
    "sam.eval()\n",
    "sam = sam.to(device)\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_msk_3D = transforms.Compose([\n",
    "FillMissingCells(desired_shape=(1,120,120,120)),\n",
    "\n",
    "])\n",
    "\n",
    "transform_3D = transforms.Compose([\n",
    "FillMissingCells(desired_shape=(1,120,120,120)),\n",
    "\n",
    "])\n",
    "transform_2d = transforms.Compose([\n",
    "\n",
    "lambda x: x.expand(3, -1, -1),\n",
    "transforms.Lambda(lambda x: x / 65535.0)\n",
    "\n",
    "])\n",
    "\n",
    "test_dataset = iqs_dv_01(data_path=os.path.join(args.data_path,'iqs_dv_01_test'),crop_size=args.crop_size, transform_3D=transform_3D, transform_msk_3D=transform_msk_3D,transform_2D=transform_2d)\n",
    "\n",
    "nice_test_loader = DataLoader(\n",
    "test_dataset,\n",
    "batch_size=args.b,\n",
    "shuffle=False,\n",
    "num_workers=args.w,\n",
    "collate_fn=collate_fn\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "\n",
    "\n",
    "# Sample image and mask tensors (replace these with your actual data)\n",
    "img_path = '/mnt/data/backup/CRT_Efficient_Annotation_SAM/iqs/deepvision/iqs_dv_08/images/000406_f54eadef-9ab3-401f-a6fb-34106eb7e18d.h5'\n",
    "with h5py.File(img_path, 'r') as img_h5:\n",
    "    image_tensor = torch.from_numpy(img_h5['data']['data'][:].astype(np.float64)).unsqueeze(0)\n",
    "    mask_tensor = torch.from_numpy(img_h5['data']['data'][:].astype(np.float64)).unsqueeze(0)\n",
    "    \n",
    "\n",
    "import numpy as np\n",
    "def interpolate(image,crop_size):\n",
    "    original_height, original_width = image.shape[-2:]\n",
    "    \n",
    "    # Use the maximum of original size and crop size for interpolation\n",
    "    interp_h = max(original_height, crop_size[0])\n",
    "    interp_w = max(original_width, crop_size[1])\n",
    "\n",
    "    # Interpolate the image using bilinear interpolation\n",
    "    image = F.interpolate(image.unsqueeze(0).unsqueeze(0), size=(interp_h, interp_w), mode='bilinear').squeeze(0).squeeze(0)\n",
    "    return image\n",
    "\n",
    "def crop(image,crop_size,top_left_x,top_left_y):\n",
    "    # Crop the image\n",
    "    return image[top_left_y:top_left_y + crop_size[0], top_left_x:top_left_x + crop_size[1]]\n",
    "\n",
    "def crop_image_and_mask(image, mask, crop_size):\n",
    "    \n",
    "    image = interpolate(image,crop_size)\n",
    "    mask = interpolate(mask,crop_size)\n",
    "    # Randomly choose the top-left corner of the crop\n",
    "    top_left_x = np.random.randint(0, image.shape[1] - crop_size[1] + 1)\n",
    "    top_left_y = np.random.randint(0, image.shape[0] - crop_size[0] + 1)\n",
    "    \n",
    "    cropped_image = crop(image, crop_size,top_left_x,top_left_y)\n",
    "    cropped_mask = crop(mask, crop_size, top_left_x, top_left_y)\n",
    "\n",
    "    return cropped_image, cropped_mask\n",
    "\n",
    "# Display the original image and mask\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(image_tensor[0,:,:,24])\n",
    "plt.title('Original Image')\n",
    "\n",
    "plt.subplot(1,4, 2)\n",
    "plt.imshow(mask_tensor[0,:,:,24])\n",
    "plt.title('Original Mask')\n",
    "\n",
    "# Define the crop size\n",
    "crop_size = (120, 50)\n",
    "\n",
    "\n",
    "# Apply transformations\n",
    "transformed_image, transformed_mask = crop_image_and_mask(image_tensor[0,:,:,24],mask_tensor[0,:,:,24],crop_size)\n",
    "\n",
    "\n",
    "# Display the transformed image and mask\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(transformed_image)\n",
    "plt.title('Transformed Image')\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(transformed_mask)\n",
    "plt.title('Transformed Mask')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_POINTS_MAX = 1 # Define the number of intial prompts generated using gaussian sampling\n",
    "N_MAX_ITER_PROMPTS = 9 # Define the number of iterative prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test easy: 100%|██████████| 33/33 [00:53<00:00,  1.61s/img]\n",
      "Test medium: 100%|██████████| 33/33 [00:48<00:00,  1.47s/img]\n",
      "Test difficult: 100%|██████████| 32/32 [00:44<00:00,  1.40s/img]\n"
     ]
    }
   ],
   "source": [
    "metrics_out = {}\n",
    "vis_path = \"home/zozchaab/vis\"\n",
    "# Iterate over the three dataset difficulties\n",
    "\n",
    "\n",
    "folder = os.path.join(SAVE_ROOT, f'{N_POINTS_MAX}_{N_MAX_ITER_PROMPTS}')\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# Create metrics\n",
    "metrics = [\n",
    "    Accuracy(task='binary').to(device), \n",
    "    Precision(task='binary').to(device), \n",
    "    Recall(task='binary').to(device), \n",
    "    F1Score(task='binary').to(device), \n",
    "    JaccardIndex(task='binary').to(device),\n",
    "]\n",
    "miou = MeanIoU('optimal', False, False).to(device)\n",
    "pq = PanopticQuality().to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    with tqdm(total=len(test_dataset), unit='img') as pbar:\n",
    "        for pack in nice_test_loader:\n",
    "            preds = []\n",
    "            scores = []\n",
    "            prompts = []\n",
    "            original_preds = []\n",
    "            imgs = pack['image'].to(dtype = torch.float32, device = device)\n",
    "            targets = pack['label'].to(dtype = torch.float32, device = device)\n",
    "            names = pack['metadata']\n",
    "            batch_loss = 0.0\n",
    "            for img, mask in zip(imgs, targets):\n",
    "                img_emb = sam.image_encoder(img.unsqueeze(0))\n",
    "                # Create Prompts\n",
    "            \n",
    "                # Randomly sample number of prompts\n",
    "                #n_points = np.random.randint(1, N_POINTS_MAX)\n",
    "                #n_pos = np.random.randint(1, n_points) if n_points > 1 else 1\n",
    "                #n_neg = np.random.randint(0, n_points-n_pos) if (n_points - n_pos) > 0 else 0\n",
    "                n_neg = 0\n",
    "                n_pos = 10\n",
    "                pts, lbls = sample_from_mask(mask.squeeze(0), mode=\"random\", n_pos=n_pos,n_neg = n_neg) \n",
    "    \n",
    "\n",
    "                user_iter = 0 \n",
    "                # Randomly add pseudo user input \n",
    "                #user_iter = np.random.randint(N_MAX_ITER_PROMPTS)\n",
    "                for i in range(user_iter):\n",
    "                    # print(f'User interaction {i+1}/{user_iter}')\n",
    "                    \n",
    "                    # Set prompt\n",
    "                    prompt = (pts.unsqueeze(0).to(device), lbls.unsqueeze(0).to(device))\n",
    "                    se, de = sam.prompt_encoder(\n",
    "                        points=prompt,\n",
    "                        boxes=None,\n",
    "                        masks=None,\n",
    "                    ) # type: ignore\n",
    "                    \n",
    "                    # Predict Mask\n",
    "                    pred, _ = sam.mask_decoder(\n",
    "                        image_embeddings=img_emb,\n",
    "                        image_pe=net.prompt_encoder.get_dense_pe(),  # type: ignore\n",
    "                        sparse_prompt_embeddings=se,\n",
    "                        dense_prompt_embeddings=de, \n",
    "                        multimask_output=False,\n",
    "                    ) # type: ignore\n",
    "                    # Compare Prediction to GT\n",
    "                    pred = F.interpolate(pred, mask.shape[-2:]) # This is a bit cumbersome, but the easiest fix for now\n",
    "                    pred = pred.squeeze() > 0 #check this!!!\n",
    "                    clusters = pred.cpu() != mask\n",
    "                    # Sample point from largest error cluster \n",
    "                    new_prompt = find_best_new_prompt(clusters)\n",
    "                    new_label = mask[new_prompt[0, 1], new_prompt[0, 0]].to(torch.int64)\n",
    "                    pts = torch.concatenate([pts, new_prompt])\n",
    "                    lbls = torch.concatenate([lbls, torch.tensor([new_label])])\n",
    "\n",
    "                # Final Mask inference\n",
    "                prompts.append([pts,lbls])\n",
    "                prompt = (pts.unsqueeze(0).to(device), lbls.unsqueeze(0).to(device))\n",
    "\n",
    "                # Set Prompt\n",
    "                \n",
    "                se, de = sam.prompt_encoder(\n",
    "                    points=prompt,\n",
    "                    boxes=None,\n",
    "                    masks=None,\n",
    "                ) # type: ignore\n",
    "\n",
    "                # Predict Mask\n",
    "                pred, score = sam.mask_decoder(\n",
    "                    image_embeddings=img_emb,\n",
    "                    image_pe=net.prompt_encoder.get_dense_pe(),  # type: ignore\n",
    "                    sparse_prompt_embeddings=se,\n",
    "                    dense_prompt_embeddings=de, \n",
    "                    multimask_output=False,\n",
    "                ) # type: ignore\n",
    "                original_preds.append((pred.squeeze(0) > 0).float())\n",
    "                pred = F.interpolate(pred, mask.shape[-2:]).squeeze(0) # This is a bit cumbersome, but the easiest fix for now\n",
    "                preds.append((pred > 0).float())\n",
    "                scores.append(score)\n",
    "                \n",
    "                for m in metrics:\n",
    "                    m.update(pred, mask.to(torch.uint8))\n",
    "                pq.update(pred, mask)\n",
    "                miou.update(pred, mask)\n",
    "                \n",
    "            scores = torch.stack(scores)\n",
    "            \n",
    "           \n",
    "            vis = True\n",
    "            if vis:\n",
    "                visualize_batch(imgs=imgs, masks=targets, pred_masks=preds, names=names, prompts=prompts,original_preds=original_preds,save_path=vis_path)\n",
    "\n",
    "            pbar.update()\n",
    "\n",
    "ms = {str(m): m.compute().item() for m in metrics}\n",
    "metrics_out = {\n",
    "    **ms,\n",
    "    'miou': {k:v.item() for k, v in miou.compute().items()},\n",
    "    'pq': {k:v.item() for k, v in pq.compute().items()},\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(os.path.join(SAVE_ROOT, f'MSA_{N_POINTS_MAX}_{N_MAX_ITER_PROMPTS}_metrics.json'), 'w') as f:\n",
    "    json.dump(metrics_out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
